{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJpFSmrOZSIA"
      },
      "source": [
        "# Math Verification Model - Binary Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hv4LG_IZSIB"
      },
      "source": [
        "## Step 1: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsbsSOekZSIC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers accelerate peft bitsandbytes datasets pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PWFdfOXZSIC"
      },
      "source": [
        "## Step 2: Import Libraries and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KXuKgVGZSIC",
        "outputId": "248e0450-ef09-4648-ef8e-9f66be9f40e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration complete\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, LlamaForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "BASE_MODEL = \"unsloth/Llama-3.2-3B-Instruct\" \n",
        "OUTPUT_DIR = \"lora_binary_classifier\"\n",
        "MAX_LENGTH = 2048\n",
        "DEFAULT_DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "\n",
        "print(\" Configuration complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYcGnlHAZSID"
      },
      "source": [
        "## Step 3: Define Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQMvCfE0ZSID",
        "outputId": "dbf3663f-2bc7-4174-be67-70ce0cc64af0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset class defined\n"
          ]
        }
      ],
      "source": [
        "class BinaryDataset(Dataset):\n",
        "    \"\"\"Binary classification dataset\"\"\"\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Build input text\n",
        "        text = f\"\"\"Verify if the following solution is correct.\n",
        "\n",
        "Question: {item['question']}\n",
        "Answer: {item['answer']}\n",
        "Solution: {item['solution']}\n",
        "\n",
        "Is this solution correct? Answer only 'yes' or 'no'.\"\"\"\n",
        "\n",
        "        # Tokenize\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=MAX_LENGTH,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Label: 0=no, 1=yes\n",
        "        label = 1 if item['is_correct'] else 0\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print(\" Dataset class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu4DnsDQZSID"
      },
      "source": [
        "## Step 4: Load and Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480,
          "referenced_widgets": [
            "212ab92b233e4782aecc4b26481a75b3",
            "2d9744dcda7a46c180857a6bd3fd3abf",
            "344243fcec054318a5cbe5c04f5deefd",
            "0a402d2b3b8e492ab347321b2535aaf2",
            "dc1e9ce9b1f54450a3f367a63fb513a6",
            "dcdaa5d384904922a0167ed9181f93a1",
            "b4670cb221fa4af49be6c893036e948e",
            "cc2afc717a724956baacb1e3630f393e",
            "5d9b7cc1fab949ef8f1eb97c6a651764",
            "82d38679fe794e879896d77574303637",
            "6710e2da1b924c159b2f59395ef4c47a",
            "e2bb2876d5b94f34bb16fbe363052d48",
            "0e54edd5675b48738bba6685e4dc1238",
            "75f1b153be8c40afa90266de0fbf8f0f",
            "3dd426ed7f614e4e98a669be49b873e4",
            "4e0288be144a44a581490571fbd9ac78",
            "d722f9eba3174ad8a1187781274b0f24",
            "eed311ad74054007b06c3b3557e4569e",
            "533a403497ab4ccb81c0b7c9329c12c0",
            "5fa5fe88ed604c96b3daef57baeaab70",
            "53e9dab1f2b24f18922b9902c7a7f0b5",
            "33addd996ebc42b1a4e59029688b8c7f",
            "cab2d7e7cc9f4fe8b3e2306bceff1ace",
            "92806e1a98394115825c9e3a5cd4ac3e",
            "37b6b49008054042b2ca2acf938f63f4",
            "7e347fbfc326435b8fc0acceb47b37a2",
            "5922d7cf9cab4937b2057769dac91a2f",
            "8e17157ed11e4792a6c8bedfc2ab3667",
            "19887686516943d08e3f02b3352c56b8",
            "66bb639b464f4f9d9f30a12a837b5fa3",
            "36111e7cd2414e778d67f1d406c45c0f",
            "f04ce96f25214b4c88708d857f05b0e5",
            "4af1160cec73434a835d90db4dd228a7",
            "abe5be994dca4ff495802a841ff1be9f",
            "dd6d711405174e03876ed02032fa13ca",
            "aa8d543b3d5343c48fd80ff8c3b6ce55",
            "37c5a983cb454c3e973aa7417b5be047",
            "b5a41433532147d29d3c89f52ac86261",
            "98b04082762f4257892b9b297c6e52ca",
            "ce1a8d2448bb40039f38a7f2923a3652",
            "180d9d00a987420780618f0c339e360f",
            "2d59500c816d486893d4a3d1446cdf9c",
            "4e59f98bd8634821903803bdaeb8591c",
            "b2dec383b5824736afae5648297b8545",
            "6f5e9b9faf1c46189cc5659fd687d7f4",
            "852c70021d1e4b0e8e60554c33adc06f",
            "9828e4fdb0324b7fa54bfda89e2f33a8",
            "f2135b6ddc514d618e538a75be6ff8c3",
            "426c25466f9042e4a8f115a566ff5f1b",
            "b52b8d9be3c14de1aaaa2bcb9e1188bb",
            "eff2bedb0ffa48b9a2d13df4c51b6363",
            "fc843e91cefb4ecd9d0ccfcea5229b99",
            "87b3f0311c0a47e09ca00823faf5df99",
            "81260d26be924c719612d5d28b3d2860",
            "c55fd33657644398979c773071ecc512",
            "00dd0f034523477e9c752e9ac5e26d7e",
            "99912c9f31b340dea24425bd38c22f2b",
            "cc5cd1fd95c44e1b87bd2197ae364109",
            "83245649b27649bf85a73559f6a20eee",
            "a40f418811974cf48b8c730dfa757ace",
            "aa2f32b22f754e27910c331d8264d931",
            "62480c3af4e14875b75c01a99dbc7cf3",
            "b3291204ff9d44c98da1b5bc8fc7f06a",
            "bcf222834f8044e18cc300df488baf8d",
            "220672d090ce4740ba9405a73d3ea044",
            "d51e6a3f1c2043439d342ffe627316af",
            "24b061706c7e496daa3e7675504101ca",
            "bd49f350e04a4cce920a7faf141bb612",
            "f5deabe5d6de4e7dbe283d40c4d4e743",
            "a3d57cc9d8024d67881e512a3a35c51e",
            "df5dcc23f25b4d48a2a4bda4b8fc0640",
            "686b516784584df38e1765d626eeb77d",
            "002c3361110144ad9fe4a19ffb52ffa1",
            "965823e89bc44bcb9eb7419185531665",
            "7b0c4ec1f82f4d1b9307792926b5a4e7",
            "acb7c8fa325040aea5ae78abd71e8cc3",
            "4b3d74ecc6d8436d976a3b11ca7efcb6"
          ]
        },
        "id": "o420Ni8uZSID",
        "outputId": "9f81bf1e-0904-4ca2-db48-6453c58f60d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loading dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "212ab92b233e4782aecc4b26481a75b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2bb2876d5b94f34bb16fbe363052d48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cab2d7e7cc9f4fe8b3e2306bceff1ace",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00001-of-00002.parquet:   0%|          | 0.00/195M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abe5be994dca4ff495802a841ff1be9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/3.65M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f5e9b9faf1c46189cc5659fd687d7f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00dd0f034523477e9c752e9ac5e26d7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original size: 1,000,000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24b061706c7e496daa3e7675504101ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After cleaning: 999,515\n",
            "\n",
            "Data preparation complete:\n",
            "  Training samples: 30,000\n",
            "  Validation samples: 2,000\n"
          ]
        }
      ],
      "source": [
        "print(\" Loading dataset...\")\n",
        "dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
        "\n",
        "print(f\"Original size: {len(dataset):,}\")\n",
        "\n",
        "# Data cleaning function\n",
        "def is_valid(example):\n",
        "    try:\n",
        "        text = str(example['question']) + str(example['solution'])\n",
        "        if len(text.strip()) < 20:\n",
        "            return False\n",
        "        solution_len = len(str(example['solution']))\n",
        "        if solution_len < 10 or solution_len > 3000:\n",
        "            return False\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Clean data\n",
        "dataset = dataset.filter(is_valid)\n",
        "print(f\"After cleaning: {len(dataset):,}\")\n",
        "\n",
        "# Split data\n",
        "shuffled = dataset.shuffle(seed=42)\n",
        "train_size = min(30000, len(shuffled) - 2000)\n",
        "\n",
        "train_data = shuffled.select(range(train_size))\n",
        "val_data = shuffled.select(range(train_size, train_size + 2000))\n",
        "\n",
        "print(f\"\\n Data preparation complete:\")\n",
        "print(f\"  Training samples: {len(train_data):,}\")\n",
        "print(f\"  Validation samples: {len(val_data):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yvMOtSXZSID"
      },
      "source": [
        "## Step 5: Load Model and Convert to Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314,
          "referenced_widgets": [
            "7318cfac1b2142c5afb9500fe59e1669",
            "a99c3f04377d4953aae4d4b920943d9a",
            "2101cfa48aa94ac199d7c78521186d93",
            "3a40b80189bd4281845c7c5cdbe0c8c8",
            "a0dfd23e10c9476cb820b3044c1cd41a",
            "4141183d5db04aafbbfcec35e8b0a896",
            "7fcff82ffc9b4b629f650aa050fb0200",
            "4f2191b9f77148fe86a7d6d420462402",
            "78945e8fbcd74a24b09883c727da454c",
            "02dfe43d8a40444db438da6055a34317",
            "2011c67f41a84b33b9ec0bec37a957f6",
            "71ff15c925dc470a951e218ba31c3d62",
            "eee3c643c0c540a68e96e8404fabcfe3",
            "55f8dd3028c04714afbc16154059c7d3",
            "f24120cb4b734b15a0c0d01d16e39d31",
            "0c7ff0d506b44f6d903ad782415298ef",
            "580cd6d5ca944edcae874a36ec5e50f3",
            "0e0b49a22e304938a8f79f327e510cbf",
            "534229ab895c47078e7a8eec5ac2bf99",
            "6b8d10d8d7c048d2ac5e3b3ad3012e8c",
            "39765cf74d8c49259cd3394ea5daea7b",
            "3440cd6e67a641898f05e47c61809479",
            "bd96916aff4e431994c44c321706ee95",
            "064682f1187b4f2ab7c23d6e1b5f7398",
            "9eb519d260804e3d826694fbb071cba6",
            "ba1cce17e53e4cfc8a0f36adb9edafd4",
            "b25c24dcdfef436099b7aeb7449fea60",
            "6d905dada8f64b0c806c6e6dc6a20daa",
            "af44587c846f4a5ead930fb73fcb8584",
            "2616d78c03fe4dac8231f5aef05febdd",
            "1ba78771e49548b683377b8358316c54",
            "6508be9d56c24b12b51b5652898a5cbf",
            "51e9d56a544c442989552634f21fd335",
            "225fa5e2fb544b8399938d6b48760553",
            "69be075cdd9c4690bfbf572fd12aebdf",
            "6f230b1a7e4a465fb06b21aae9f52495",
            "7fca34732f184f26b2671f291fe7c7e5",
            "c240edbafe26420cb6ae0d55aa038124",
            "543fc42131414d28b30c9a81a2d3a8f9",
            "5c0ee39e775346dfaa8133a2f47b02fc",
            "bd220fb708aa452083cb8a66c6f4fb89",
            "aa08e795faec44a0ae58fc0e75a5a157",
            "2aa7b5a7872348ea9257bdfb9364c8ee",
            "49edc66f06e34154bc5a976339fd2e75",
            "90e93775b33b4faab7735794809293ed",
            "022781843ada48babb78d6ac0f5d90ae",
            "d1e8ede2cc9043258b5bc8bf59bd9aa2",
            "9122395d666844c1abe8e115fe62530f",
            "e98072d1f73546abbaf0d7cd0fbdd812",
            "0ea1d336e6c84b36bd38ccc55d5cc371",
            "43cbcbb6f3e84f84b8020cc553111b55",
            "ace127aae0804e31830a19888fc1b4c3",
            "48a9a2a38940498791f2be6317d8e24a",
            "e18fcc4e727c43fda0f4bc366f5766e3",
            "7acfbc967bf846c79969c6749be528be",
            "dc9c85eddc0e4767ba85f717950f3ea1",
            "4a1951677de74f40910f66f94c9fc466",
            "57decbaf3dea4107972f20c2db121adf",
            "1e60ef2a00d64e7083752b561a9c677a",
            "f70f5e8770ab44a4881c84b4ed53c123",
            "3128fd3f1cb3426194c28b60d8a8debd",
            "83dba063b4674c00be4e9d8a9c3b0ee8",
            "71a05cf27d8e4a4084f1e33fbe79a8bf",
            "744fa2098585423d98f1bd86ff7a4049",
            "98b4a240053945c2bf4a1e76f24a46f4",
            "bd5bf6dcf5b042968f38d752b35517b5",
            "48b823019fa040cbba76e9b9a1e768d9",
            "225306925e7c4493b0f1acd65baa862d",
            "cc2ad7f1bf3d4984992bfee293d77ef7",
            "46cf3c525d69414988dad75246bca15c",
            "56442e64ea6746309f6d7048a0e38f7f",
            "3a8231a670684b63962b21db3b14c019",
            "4e487d8d13f84f8896d4c28f442abbe8",
            "4a385214322c432a93d6d3bc2dce8cb4",
            "180a00c6b3a34f69a0099ea4e98c7b1b",
            "70d3c17bfb5642bab59c11c42982ced8",
            "d5ea9c27836b4721af4f68e45f73c70e"
          ]
        },
        "id": "-nyj2wOOZSID",
        "outputId": "2ca86c9a-0a01-4b80-ea46-60943e5b1aaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loading model: unsloth/Llama-3.2-3B-Instruct\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7318cfac1b2142c5afb9500fe59e1669",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71ff15c925dc470a951e218ba31c3d62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd96916aff4e431994c44c321706ee95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "225fa5e2fb544b8399938d6b48760553",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90e93775b33b4faab7735794809293ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc9c85eddc0e4767ba85f717950f3ea1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48b823019fa040cbba76e9b9a1e768d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting model to binary classification...\n",
            "Model modification complete\n"
          ]
        }
      ],
      "source": [
        "print(f\" Loading model: {BASE_MODEL}\")\n",
        "\n",
        "# Load model\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=DEFAULT_DTYPE,\n",
        "    device_map=\"auto\",\n",
        "\n",
        ")\n",
        "# Key: Convert lm_head to 2-class classifier\n",
        "print(\" Converting model to binary classification...\")\n",
        "model.lm_head = torch.nn.Linear(\n",
        "    model.config.hidden_size,\n",
        "    2,  # yes=1, no=0\n",
        "    bias=False,\n",
        "    device=model.lm_head.weight.device,\n",
        "    dtype=model.lm_head.weight.dtype\n",
        ")\n",
        "model.config.vocab_size = 2\n",
        "\n",
        "# Freeze all base model parameters\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(\" Model modification complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKO9LeXxZSID"
      },
      "source": [
        "## Step 6: Add LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTKQGHNjZSIE",
        "outputId": "4bd96a66-90f0-4a30-e7d3-7fd7b1358cb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding LoRA layers...\n",
            "trainable params: 25,696,256 || all params: 3,238,452,224 || trainable%: 0.7935\n",
            "LoRA added\n"
          ]
        }
      ],
      "source": [
        "print(\" Adding LoRA layers...\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=64,  # LoRA rank\n",
        "    inference_mode=False,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],  # Only fine-tune attention\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0,\n",
        "    modules_to_save=[\"lm_head\"],  \n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\" LoRA added\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-EiARygZSIE"
      },
      "source": [
        "## Step 7: Prepare Tokenizer and Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "0751e09d779d408b8bf85751eb9b11b6",
            "944f89bf74c14d51ba89ce3787b31e26",
            "88f99b219fd84c93ac74fbe6238067ce",
            "620990b0cd0546e398997aded68e7ad2",
            "30f75d2614834fdda5cf591666446f19",
            "391047d89ede4174b9087f7f34da09f7",
            "c472997c73904a1690b1a971731a7073",
            "5f3e0ebb80854a93a89be9766744e1bb",
            "da8b8103bfcc479b9c606e12c1a1dce2",
            "0d14a316e2924154a3b47ffd89e6f6cd",
            "9395e3b90d854474bc607bdc33d71c7d",
            "4d262f59cea542938099dfbbc57ae353",
            "74f0cb6a836541f880f47b10206683ec",
            "68a46be5a2be4a58843a6ffa0047ed73",
            "e336a90d2b1f4208aa3f5ac0567d38ec",
            "ed1b5f8d88824938925b38ea4fe757ff",
            "48c502e5d1dd4f06821b3f4d910668c3",
            "2e760b123f8f4f2e9b60c0bdcff77a23",
            "88bac34b86c24b9e937e4582e8d4cf5a",
            "2d7c8be526604ae0a16c81a6f4e26aeb",
            "5b2130499b324b71a74115e3f39cbbf5",
            "4f9069072496433c8746bcc5cfd393da",
            "7c8e89a49eb14561979337dcdb807a62",
            "264866b4c84f4246aa8003b00ea77fb4",
            "ff70d4dcbe6d4e9681767e2af72b17bb",
            "d9f1a654262d4dad83b631643c32d7bd",
            "83cdc17a53af4db992ef1916c28923d1",
            "15832a0d420d435fb9ec4cbd7becc4f5",
            "6524d04b4d4e4d7a90a46eac7a4f91b8",
            "852bb7ca37f6447f9fc03a0465d1d737",
            "8a01c6fe305b4b778350ab17339d1e45",
            "c28ef03c6ad94c68b1c40be18153bf21",
            "fe59cc6684de4b799a9233f2285a7b57",
            "1c0dc12ec04c4b6e899392615b11d473",
            "b7f7e286bff945dca37f3dff813a78b9",
            "e343ad00f92847c4acb444e5bfa17290",
            "b3de194bae534251b572c52d23673032",
            "ffcf6a0fd03340b0b0f7a4336e6c73b0",
            "2b5ed4735d8d4d5286afef375d65bf9f",
            "3c397ef8b42d4b40bdd8daccb0d265db",
            "c5cc8170c0d747608e722b3b7b5469a7",
            "4e3bf34114494a9b8ae37e448e1ce381",
            "81f35435424041be956c657b339a997b",
            "6456969fea9241f3939519650e337c19"
          ]
        },
        "id": "YLI89kZXZSIE",
        "outputId": "8b95a6ab-372c-482a-95e4-7b3d02c5ce43"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0751e09d779d408b8bf85751eb9b11b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d262f59cea542938099dfbbc57ae353",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c8e89a49eb14561979337dcdb807a62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c0dc12ec04c4b6e899392615b11d473",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer and datasets ready\n",
            "  Training set size: 30,000\n",
            "  Validation set size: 2,000\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = BinaryDataset(train_data, tokenizer)\n",
        "val_dataset = BinaryDataset(val_data, tokenizer)\n",
        "\n",
        "print(f\" Tokenizer and datasets ready\")\n",
        "print(f\"  Training set size: {len(train_dataset):,}\")\n",
        "print(f\"  Validation set size: {len(val_dataset):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMSn9LqaZSIE"
      },
      "source": [
        "## Step 8: Define Custom Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziDCX7LfZSIE",
        "outputId": "d9d0ab59-2dff-4f03-8484-3b466d9a29b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainer class defined\n"
          ]
        }
      ],
      "source": [
        "class BinaryClassificationTrainer(Trainer):\n",
        "    \"\"\"Binary classification trainer\"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Take logits from last token\n",
        "        logits = logits[:, -1, :]  # [batch_size, 2]\n",
        "\n",
        "        # Compute cross-entropy loss\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute accuracy\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    # Take predictions from last token\n",
        "    logits = logits[:, -1, :]\n",
        "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
        "    accuracy = (predictions == torch.tensor(labels)).float().mean().item()\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "\n",
        "print(\" Trainer class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqnVLv3DZSIE"
      },
      "source": [
        "## Step 9: Configure Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHyZY55OZSIE",
        "outputId": "7bc8f8dc-b391-4819-98c4-da8ffd0bdc5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainer configured\n",
            "  Estimated training time: ~60-70 minutes\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,  # Proven configuration: 1 epoch\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    warmup_steps=200,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    optim=\"adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "trainer = BinaryClassificationTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\" Trainer configured\")\n",
        "print(f\"  Estimated training time: ~60-70 minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58cH1lIsZSIE"
      },
      "source": [
        "## Step 10: Start Training\n",
        "\n",
        "â± **Estimated time: 60-70 minutes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6OmBtdNaZSIE",
        "outputId": "a66e375b-f315-425f-8112-733184d4d715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1054' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1054/1875 1:30:37 < 1:10:43, 0.19 it/s, Epoch 0.56/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.988500</td>\n",
              "      <td>0.587018</td>\n",
              "      <td>0.775500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.639100</td>\n",
              "      <td>0.476464</td>\n",
              "      <td>0.833500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.383700</td>\n",
              "      <td>0.350486</td>\n",
              "      <td>0.843000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.555000</td>\n",
              "      <td>0.325677</td>\n",
              "      <td>0.864000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.177900</td>\n",
              "      <td>0.285539</td>\n",
              "      <td>0.876000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1875/1875 2:41:48, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.988500</td>\n",
              "      <td>0.587018</td>\n",
              "      <td>0.775500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.639100</td>\n",
              "      <td>0.476464</td>\n",
              "      <td>0.833500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.383700</td>\n",
              "      <td>0.350486</td>\n",
              "      <td>0.843000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.555000</td>\n",
              "      <td>0.325677</td>\n",
              "      <td>0.864000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.177900</td>\n",
              "      <td>0.285539</td>\n",
              "      <td>0.876000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.163500</td>\n",
              "      <td>0.286691</td>\n",
              "      <td>0.875500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.980500</td>\n",
              "      <td>0.275313</td>\n",
              "      <td>0.887000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.023500</td>\n",
              "      <td>0.253447</td>\n",
              "      <td>0.896000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.963000</td>\n",
              "      <td>0.240893</td>\n",
              "      <td>0.900500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: lora_binary_classifier\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\" Training complete!\\n\")\n",
        "\n",
        "# Save model\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\" Model saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2Q6BqwEZSIE"
      },
      "source": [
        "## Step 11: Generate Test Set Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KEEX8apZSIE",
        "outputId": "8e3f3c34-6175-4017-cb6f-610657f29fa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating test set predictions...\n",
            "\n",
            "Test set size: 10,000\n",
            "Processing: 0/10000\n",
            "Processing: 100/10000\n",
            "Processing: 200/10000\n",
            "Processing: 300/10000\n",
            "Processing: 400/10000\n",
            "Processing: 500/10000\n",
            "Processing: 600/10000\n",
            "Processing: 700/10000\n",
            "Processing: 800/10000\n",
            "Processing: 900/10000\n",
            "Processing: 1000/10000\n",
            "Processing: 1100/10000\n",
            "Processing: 1200/10000\n",
            "Processing: 1300/10000\n",
            "Processing: 1400/10000\n",
            "Processing: 1500/10000\n",
            "Processing: 1600/10000\n",
            "Processing: 1700/10000\n",
            "Processing: 1800/10000\n",
            "Processing: 1900/10000\n",
            "Processing: 2000/10000\n",
            "Processing: 2100/10000\n",
            "Processing: 2200/10000\n",
            "Processing: 2300/10000\n",
            "Processing: 2400/10000\n",
            "Processing: 2500/10000\n",
            "Processing: 2600/10000\n",
            "Processing: 2700/10000\n",
            "Processing: 2800/10000\n",
            "Processing: 2900/10000\n",
            "Processing: 3000/10000\n",
            "Processing: 3100/10000\n",
            "Processing: 3200/10000\n",
            "Processing: 3300/10000\n",
            "Processing: 3400/10000\n",
            "Processing: 3500/10000\n",
            "Processing: 3600/10000\n",
            "Processing: 3700/10000\n",
            "Processing: 3800/10000\n",
            "Processing: 3900/10000\n",
            "Processing: 4000/10000\n",
            "Processing: 4100/10000\n",
            "Processing: 4200/10000\n",
            "Processing: 4300/10000\n",
            "Processing: 4400/10000\n",
            "Processing: 4500/10000\n",
            "Processing: 4600/10000\n",
            "Processing: 4700/10000\n",
            "Processing: 4800/10000\n",
            "Processing: 4900/10000\n",
            "Processing: 5000/10000\n",
            "Processing: 5100/10000\n",
            "Processing: 5200/10000\n",
            "Processing: 5300/10000\n",
            "Processing: 5400/10000\n",
            "Processing: 5500/10000\n",
            "Processing: 5600/10000\n",
            "Processing: 5700/10000\n",
            "Processing: 5800/10000\n",
            "Processing: 5900/10000\n",
            "Processing: 6000/10000\n",
            "Processing: 6100/10000\n",
            "Processing: 6200/10000\n",
            "Processing: 6300/10000\n",
            "Processing: 6400/10000\n",
            "Processing: 6500/10000\n",
            "Processing: 6600/10000\n",
            "Processing: 6700/10000\n",
            "Processing: 6800/10000\n",
            "Processing: 6900/10000\n",
            "Processing: 7000/10000\n",
            "Processing: 7100/10000\n",
            "Processing: 7200/10000\n",
            "Processing: 7300/10000\n",
            "Processing: 7400/10000\n",
            "Processing: 7500/10000\n",
            "Processing: 7600/10000\n",
            "Processing: 7700/10000\n",
            "Processing: 7800/10000\n",
            "Processing: 7900/10000\n",
            "Processing: 8000/10000\n",
            "Processing: 8100/10000\n",
            "Processing: 8200/10000\n",
            "Processing: 8300/10000\n",
            "Processing: 8400/10000\n",
            "Processing: 8500/10000\n",
            "Processing: 8600/10000\n",
            "Processing: 8700/10000\n",
            "Processing: 8800/10000\n",
            "Processing: 8900/10000\n",
            "Processing: 9000/10000\n",
            "Processing: 9100/10000\n",
            "Processing: 9200/10000\n",
            "Processing: 9300/10000\n",
            "Processing: 9400/10000\n",
            "Processing: 9500/10000\n",
            "Processing: 9600/10000\n",
            "Processing: 9700/10000\n",
            "Processing: 9800/10000\n",
            "Processing: 9900/10000\n",
            "\n",
            "Predictions complete.\n"
          ]
        }
      ],
      "source": [
        "print(\" Generating test set predictions...\\n\")\n",
        "\n",
        "# Load test set\n",
        "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
        "print(f\"Test set size: {len(test_dataset):,}\")\n",
        "\n",
        "# Predict\n",
        "predictions = []\n",
        "model.eval()\n",
        "\n",
        "for i, item in enumerate(test_dataset):\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Processing: {i}/{len(test_dataset)}\")\n",
        "\n",
        "    # Build input\n",
        "    text = f\"\"\"Verify if the following solution is correct.\n",
        "\n",
        "Question: {item['question']}\n",
        "Answer: {item['answer']}\n",
        "Solution: {item['solution']}\n",
        "\n",
        "Is this solution correct? Answer only 'yes' or 'no'.\"\"\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=MAX_LENGTH, truncation=True)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits[:, -1, :]  # Take last token\n",
        "        prediction = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    predictions.append(prediction)\n",
        "\n",
        "print(f\"\\n Predictions complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvQ5hWBwZSIE"
      },
      "source": [
        "## Step 12: Create Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt7CEp33ZSIE",
        "outputId": "4decfe7c-ab8b-41ad-c965-df52ca39639e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission file saved: submission.csv\n",
            "\n",
            "Prediction statistics:\n",
            "  Predicted as correct: 3,659\n",
            "  Predicted as incorrect: 6,341\n",
            "  Correct ratio: 36.6%\n",
            "\n",
            "Submission file preview:\n",
            "   ID  is_correct\n",
            "0   0       False\n",
            "1   1       False\n",
            "2   2       False\n",
            "3   3        True\n",
            "4   4       False\n",
            "5   5       False\n",
            "6   6       False\n",
            "7   7       False\n",
            "8   8       False\n",
            "9   9       False\n",
            "\n",
            "Download submission.csv and submit to Kaggle\n"
          ]
        }
      ],
      "source": [
        "# Create submission file\n",
        "submission = pd.DataFrame({\n",
        "    'ID': range(len(predictions)),  # Uppercase ID for Kaggle\n",
        "    'is_correct': [bool(p) for p in predictions]  # True/False format\n",
        "})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\" Submission file saved: submission.csv\")\n",
        "print(f\"\\n Prediction statistics:\")\n",
        "print(f\"  Predicted as correct: {sum(predictions):,}\")\n",
        "print(f\"  Predicted as incorrect: {len(predictions) - sum(predictions):,}\")\n",
        "print(f\"  Correct ratio: {sum(predictions)/len(predictions)*100:.1f}%\")\n",
        "\n",
        "# Display preview\n",
        "print(f\"\\nSubmission file preview:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" All complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nDownload submission.csv and submit to Kaggle\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}